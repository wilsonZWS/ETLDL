import unittest
from tensorflow import keras
import tensorflow as tf


class configObjTestCase(unittest.TestCase):
    def testCuDNN(self):
        batch_size = 64
        # Each MNIST image batch is a tensor of shape (batch_size, 28, 28).
        # Each input sequence will be of size (28, 28) (height is treated like time).
        input_dim = 28

        units = 64
        output_size = 10  # labels are from 0 to 9

        # Build the RNN model
        def build_model(allow_cudnn_kernel=True):
            # CuDNN is only available at the layer level, and not at the cell level.
            # This means `LSTM(units)` will use the CuDNN kernel,
            # while RNN(LSTMCell(units)) will run on non-CuDNN kernel.
            if allow_cudnn_kernel:
                # The LSTM layer with default options uses CuDNN.
                lstm_layer = keras.layers.LSTM(units, input_shape=(None, input_dim))
            else:
                # Wrapping a LSTMCell in a RNN layer will not use CuDNN.
                lstm_layer = keras.layers.RNN(
                    keras.layers.LSTMCell(units), input_shape=(None, input_dim)
                )
            model = keras.models.Sequential(
                [
                    lstm_layer,
                    keras.layers.BatchNormalization(),
                    keras.layers.Dense(output_size),
                ]
            )
            return model

        mnist = keras.datasets.mnist
        (x_train, y_train), (x_test, y_test) = mnist.load_data()
        x_train, x_test = x_train / 255.0, x_test / 255.0
        sample, sample_label = x_train[0], y_train[0]

        model = build_model(allow_cudnn_kernel=True)

        model.compile(
            loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),
            optimizer="sgd",
            metrics=["accuracy"],
        )

        model.fit(
            x_train, y_train, validation_data=(x_test, y_test), batch_size=batch_size, epochs=1
        )

        noncudnn_model = build_model(allow_cudnn_kernel=False)
        noncudnn_model.set_weights(noncudnn_model.get_weights())
        noncudnn_model.compile(
            loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),
            optimizer="sgd",
            metrics=["accuracy"],
        )
        noncudnn_model.fit(
            x_train, y_train, validation_data=(x_test, y_test), batch_size=batch_size, epochs=1
        )

        import matplotlib.pyplot as plt

        with tf.device("CPU:0"):
            cpu_model = build_model(allow_cudnn_kernel=True)
            cpu_model.set_weights(noncudnn_model.get_weights())
            result = tf.argmax(cpu_model.predict_on_batch(tf.expand_dims(sample, 0)), axis=1)
            print(
                "Predicted result is: %s, target result is: %s" % (result.numpy(), sample_label)
            )
            plt.imshow(sample, cmap=plt.get_cmap("gray"))


    def testfastcude(self):
        tf.test.is_built_with_cuda()
        tf.test.is_gpu_available(cuda_only=False, min_cuda_compute_capability=None)